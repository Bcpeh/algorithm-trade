{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import alpaca_trade_api as tradeapi\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pehbo\\projects\\algo\n"
     ]
    }
   ],
   "source": [
    "os.chdir('../')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your own API key and secret\n",
    "with open('secrets/secrets.json') as f:\n",
    "    secrets = json.load(f)\n",
    "API_KEY = secrets['KEY']\n",
    "API_SECRET = secrets['SECRET']\n",
    "BASE_URL = 'https://paper-api.alpaca.markets'  # For paper trading, use the paper trading URL\n",
    "\n",
    "# Set up the Alpaca API client\n",
    "api = tradeapi.REST(API_KEY, API_SECRET, base_url=BASE_URL, api_version='v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = 'TSLA'\n",
    "start_date = '2010-01-01'\n",
    "end_date = '2023-03-25'\n",
    "\n",
    "historical_data = api.get_bars(symbol, tradeapi.rest.TimeFrame.Day, start=start_date, end=end_date).df\n",
    "historical_data.index.name = 'date'\n",
    "historical_data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the TSLA news data\n",
    "tsla_news = api.get_news(symbol, start_date, end_date)\n",
    "news_summaries = [news.summary for news in tsla_news]\n",
    "news_dates = [news.updated_at.date() for news in tsla_news]\n",
    "# Extract significant words and phrases using CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=50)\n",
    "X = vectorizer.fit_transform(news_summaries).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "from dateutil.parser import parse\n",
    "\n",
    "# Fetch news summaries from Yahoo News\n",
    "yahoo_news_rss_url = 'https://news.yahoo.com/rss/tesla'\n",
    "parsed_rss = feedparser.parse(yahoo_news_rss_url)\n",
    "news_titles = [entry.title for entry in parsed_rss.entries]\n",
    "news_dates = [parse(news.published).date() for news in parsed_rss.entries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the news data\n",
    "def preprocess_text(text):\n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    \n",
    "    # Convert text to lowercase and tokenize\n",
    "    words = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "cleaned_titles = [preprocess_text(title) for title in news_titles]\n",
    "# Extract significant words and phrases using CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=50)\n",
    "X = vectorizer.fit_transform(cleaned_titles).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to store significant words and phrases\n",
    "significant_words = pd.DataFrame(X, columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Merge the TSLA historical data and significant_words DataFrames\n",
    "historical_data['date'] = pd.to_datetime(historical_data['date']).dt.date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted next closing price for TSLA: 190.40999999999985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pehbo\\anaconda3\\envs\\algo\\lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "significant_words['date'] = news_dates\n",
    "merged_data = pd.merge(historical_data, significant_words, on='date', how='inner')\n",
    "\n",
    "# Train a linear regression model to predict the next closing price\n",
    "X_train = merged_data.drop(['date', 'close'], axis=1)\n",
    "y_train = merged_data['close']\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the next closing price\n",
    "X_test = X_train.iloc[-1].values.reshape(1, -1)\n",
    "next_closing_price = lr_model.predict(X_test)\n",
    "\n",
    "print(\"Predicted next closing price for TSLA:\", next_closing_price[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Search on for missing in deadly chocolate factory explosion',\n",
       " 'title_detail': {'type': 'text/plain',\n",
       "  'language': None,\n",
       "  'base': 'https://news.yahoo.com/rss/tesla',\n",
       "  'value': 'Search on for missing in deadly chocolate factory explosion'},\n",
       " 'links': [{'rel': 'alternate',\n",
       "   'type': 'text/html',\n",
       "   'href': 'https://news.yahoo.com/six-injured-blast-pennsylvania-chocolate-013258990.html'}],\n",
       " 'link': 'https://news.yahoo.com/six-injured-blast-pennsylvania-chocolate-013258990.html',\n",
       " 'published': '2023-03-25T01:32:58Z',\n",
       " 'published_parsed': time.struct_time(tm_year=2023, tm_mon=3, tm_mday=25, tm_hour=1, tm_min=32, tm_sec=58, tm_wday=5, tm_yday=84, tm_isdst=0),\n",
       " 'source': {'href': 'http://www.ap.org/', 'title': 'Associated Press'},\n",
       " 'id': 'six-injured-blast-pennsylvania-chocolate-013258990.html',\n",
       " 'guidislink': False,\n",
       " 'media_content': [{'height': '86',\n",
       "   'url': 'https://media.zenfs.com/en/ap.org/4cd97a6c150225c1b45daadb58131e24',\n",
       "   'width': '130'}],\n",
       " 'media_credit': [{'role': 'publishing company'}],\n",
       " 'credit': ''}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_rss.entries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted next closing price for TSLA based on current latest news: 190.40999999999985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pehbo\\anaconda3\\envs\\algo\\lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Fetch the latest news title from Yahoo News\n",
    "latest_news_title = parsed_rss.entries[0].title\n",
    "\n",
    "# Preprocess the latest news title\n",
    "cleaned_latest_title = preprocess_text(latest_news_title)\n",
    "\n",
    "# Transform the preprocessed title into a feature vector\n",
    "latest_title_vector = vectorizer.transform([cleaned_latest_title]).toarray()\n",
    "\n",
    "# Create a DataFrame for the latest news title\n",
    "latest_significant_words = pd.DataFrame(latest_title_vector, columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Ensure the latest_significant_words DataFrame has the same columns as the significant_words DataFrame\n",
    "for col in significant_words.columns:\n",
    "    if col not in latest_significant_words.columns:\n",
    "        latest_significant_words[col] = 0\n",
    "\n",
    "latest_significant_words = latest_significant_words[significant_words.columns]\n",
    "\n",
    "# Create a new DataFrame with the same columns as the training data\n",
    "latest_merged_data = pd.DataFrame(columns=X_train.columns)\n",
    "\n",
    "# Fill the DataFrame with values from the last row of historical data and the latest_significant_words DataFrame\n",
    "for col in latest_merged_data.columns:\n",
    "    if col in latest_significant_words.columns:\n",
    "        latest_merged_data.loc[0, col] = latest_significant_words.loc[0, col]\n",
    "    else:\n",
    "        latest_merged_data.loc[0, col] = merged_data.iloc[-1][col]\n",
    "\n",
    "# Predict the next closing price based on the current latest news\n",
    "X_test_latest = latest_merged_data.values.reshape(1, -1)\n",
    "next_closing_price_latest = lr_model.predict(X_test_latest)\n",
    "\n",
    "print(\"Predicted next closing price for TSLA based on current latest news:\", next_closing_price_latest[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import schedule\n",
    "import time\n",
    "\n",
    "def update_model():\n",
    "    # Fetch the latest TSLA news data\n",
    "    parsed_rss = feedparser.parse(yahoo_news_rss_url)\n",
    "    latest_news_summary = parsed_rss.entries[0].summary\n",
    "\n",
    "    # Preprocess the latest news summary\n",
    "    cleaned_latest_summary = preprocess_text(latest_news_summary)\n",
    "\n",
    "    # Transform the preprocessed summary into a feature vector\n",
    "    latest_summary_vector = vectorizer.transform([cleaned_latest_summary]).toarray()\n",
    "\n",
    "    # Create a DataFrame for the latest news summary\n",
    "    latest_significant_words = pd.DataFrame(latest_summary_vector, columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    # Fetch the latest TSLA historical data\n",
    "    tsla_data = api.get_barset('TSLA', 'day', limit=1).df['TSLA'].reset_index()\n",
    "\n",
    "    # Merge the latest TSLA historical data and latest_significant_words DataFrames\n",
    "    tsla_data['date'] = pd.to_datetime(tsla_data['time']).dt.date\n",
    "    latest_significant_words['date'] = parsed_rss.entries[0].published_parsed[:3]\n",
    "    latest_merged_data = pd.merge(tsla_data, latest_significant_words, on='date', how='inner')\n",
    "\n",
    "    # Update the training data and retrain the linear regression model\n",
    "    global merged_data, X_train, y_train, lr_model\n",
    "    merged_data = merged_data.append(latest_merged_data, ignore_index=True)\n",
    "    X_train = merged_data.drop(['date', 'close'], axis=1)\n",
    "    y_train = merged_data['close']\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    print(\"Model updated with latest data.\")\n",
    "\n",
    "# Schedule the update_model function to run daily\n",
    "schedule.every().day.at(\"18:00\").do(update_model)\n",
    "\n",
    "# Keep the script running indefinitely\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
